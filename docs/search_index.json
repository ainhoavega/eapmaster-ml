[["index.html", "Empirical Applications with Big Data Tools Preface", " Empirical Applications with Big Data Tools Ainhoa Vega Bayo 2023-01-18 Preface These slides contain notes and assignments for the ‘’Empirical Applications with Big Data Tools’’ part of the Master in Economics: Empirical Applications and Policies at the University of the Basque Country UPV/EHU. These are intended to introduce social scientists to concepts in Big Data and Machine Learning using R. We will be using DataCamp’s interactive classroom features to practice all coding exercises. Special thanks to Petr Mariel (UPV/EHU) and David Martínez (ISEAK) for their input. "],["what-is-big-data.html", "Chapter 1 What is Big Data? 1.1 The three V’s of Big Data 1.2 Other V’s of Big Data 1.3 Some statistics of Big Data 1.4 Types of Big Data 1.5 How is Big Data being used? 1.6 Concerns", " Chapter 1 What is Big Data? Big Data is a collection of data that is huge in volume, yet growing exponentially with time. It is a data with so large size and complexity that none of traditional data management tools can store it or process it efficiently. It refers to massive, complex data sets (either structured, semi-structured or unstructured) that are rapidly generated and transmitted from a wide variety of sources. In 2023, we’re surrounded by examples of Big Data: Social Media Stock Exchanges Flights … basically anything you can think of! 1.1 The three V’s of Big Data Volume. The huge amounts of data being stored. Velocity. The speed at which (new) data streams must be processed and analyzed. Variety. The different sources and forms from which data is obtained, e.g. numbers, text, images, video, audio, … Volume As of 2021, 2.5 quintillion bytes of data are produced by humans every single day. A quintillion is a number with 18 zeros. It is estimated that 463 exabytes of data will be generated by 5 billion internet users all over the world by the year 2025 (source). Big Data is all about volume. Source. Because there is so much data available, businesses can use both past and present data to gain insights into their customers in a more comprehensive way. With such massive amounts of data, new and innovative data processing and storing technologies are required. These datasets just can’t be processed by a conventional desktop computer and processor due to their size. There were 5 exabytes of information created between the dawn of civilization through 2003, but that much information is now created every two days. Eric Schmidt, Executive Chairman at Google (in 2010!) Velocity The pace at which Big Data is created, gathered, and distributed is referred to as velocity. This is the rapid rate at which data is generated from various sources, including machines, corporate operations, and user interactions with social media platforms, among others. High-velocity data processing calls for specialized methods using cutting-edge analytical and algorithmic tools. The speed at which data is created becomes even more crucial for some applications than the volume itself. Fast-moving data is advantageous for financial trading companies. For example, Amazon records every mouse click made by customers while browsing its website. This happens quickly. Stream processing relies heavily on velocity. Consider all of the data coming in from radio-frequency identification (RFID), global positioning system (GPS), near-field communication (NFC), and Bluetooth sensors. When a specific pattern is detected, stream processing attempts to aggregate single data points from high-velocity data in order to trigger a high-level event. It also focuses on deciding which data to keep from a stream, because it is impossible to keep all of the data that comes in. Source. Variety There are a number of relatively recent sources for gathering both structured and unstructured data, including sensor readings, photographs on social media, GPS signals from mobile devices, and more. Smartphones and other mobile devices are now an important source of information on people, activity, and locations due to the global surge in internet use. Structured data, which is simpler to store, is made up of the conventionally available data, such as spreadsheets and text. It is challenging to store and evaluate the vast types of unstructured data, such as videos, photos, and audios, in order to come up with findings. Finding data that is well organized and easy to process is quite unusual. 1.2 Other V’s of Big Data Although the previously mentioned V’s are the three main V’s of Big Data, two additional ones are typically considered as well, depending on the source: Veracity. Refers to the quality and accuracy of data. Value. Refers to the insights that big data can provide. Veracity The collected data can be incomplete, erroneous, or unable to offer any useful, insightful information. Veracity, in general, refers to the degree of confidence in the data that has been gathered. Data can occasionally become disorganized and challenging to use. If the data is incomplete, a big volume of data may produce more confusion than insights. For instance, in the medical industry, if information about the medications a patient is taking is lacking, the patient’s life may be in danger. Questions to ask: are we missing some (key) pieces of data? Are the data that we have clean and accurate? Value Data is useless unless it can be used to generate actionable insights that help a business or organization grow. The data value is a concept that is frequently quantified as the potential economic value of the data. The value of the same data may differ from one organization to the next[8]. For example, an app like Google Maps may use GPS data from a mobile phone to calculate a navigation route. The same GPS data can be used by a fitness app like Apple Health to calculate the number of steps taken and calories burned. As a result, the concept of data value is weakly defined, however, in terms of business, the V for value sits at the top of the Big Data pyramid. It basically refers to the ability to transform a tsunami of data into insights, which can be used for business. Value refers to how useful the data is in decision making. We need to extract the value of the Big Data using proper analytics. Real-life example from an insurance company: We can also localize our most important customers, and we know which ones do not have any value, either because they cancel frequently, are always looking for discounts, or we may have suspicions of fraud. They are customers with a similar profile, but they’re also very different. Years ago, we weren’t able to distinguish them. Now we can, thanks to big data. … and even more V’s of Big Data Should it be 3 V’s? 5 V’s? 10? 17? 1.3 Some statistics of Big Data Some interesting statistics on Big Data (source, Dec 2022): In 2021, it was forecast that the overall amount of data created worldwide would reach 79 zettabytes (1 zettabyte = 1,000,000,000 terabytes). By 2025, this amount is expected to double. There is 10% unique and 90 % replicated data in the global datasphere. Of all of the data in the world at the moment, approximately 90% of it is replicated data, with only 10% being genuine, new data. The 1:9 ratio is expected to change (worsen) to 1:10 by 2024. It would take approximately 181 million years for an internet user to download all data from the web today. Between 80-90% of the data that internet users generate in unstructured. In 2021, the US was the country with most data centers in the world. 1.4 Types of Big Data Big data is classified in three ways: Structured data Unstructured data Semi-structured data While these three terms are technically applicable to all amounts of datag, they are especially important in big data. Understanding where raw data comes from and how it must be treated before analysis becomes even more important when dealing with large amounts of big data. Because there is so much of it, information extraction must be efficient in order to be worthwhile. The structure of the data determines not only how to work with it, but also what insights it can yield. Before data can be analyzed, it must first be extracted, transformed, and loaded (ETL). It’s a very literal term: data is harvested, formatted so that an application can read it, and then stored for later use. The ETL process differs for each data structure. Structured data Structured data is the one we’re used to and the one we typically work with. It is well-organized, with dimensions determined by predefined parameters. Consider spreadsheets, STATA, R; each piece of information is organized into rows and columns. Specific elements defined by specific variables are easily found. It’s the quantitative and qualitative data we typically think of: Age Sex Nationality Income Address Bank account numbers … Because structured data is already tangible numbers, a program can sort through and collect data much more easily. Structured data adheres to schemas, which are essentially road maps to specific data points. These schemas describe where each datum is located and what it means. For example, a payroll database will include information such as employee identification, pay rate, hours worked, and how compensation is delivered, among other things. Each of these dimensions will be defined by the schema for whatever application is using it. The program will not have to sift through data to figure out what it means; instead, it will be able to get right to work collecting and processing it. Unstructured data On the other hand, unstructured data (which accounts for up to 80-90% of all data) is any piece of information that lacks a specific format and is stored in original form without any presets or schemas. It doesn’t fit into these sorts of pre-defined data models. It can’t be stored neatly into columns and rows. And because it comes in so many formats, it’s a real challenge for conventional software to ingest, process, and analyze. Or to be extracted, transformed, and loaded (ETL). Simple content searches can be undertaken across textual unstructured data with the right tools, but beyond that, the lack of consistent internal structure doesn’t conform to what typical data mining systems can work with. Some examples of unstructured data: Multimedia content: Digital photos, audio, and video files are all unstructured. Complicating matters, multimedia can come in multiple format files, produced through various means. For instance, a photo can be TIFF, JPEG, GIF, PNG, or RAW, each with their own characteristics. Text files: Almost all traditional business files, including your word processing documents, presentations, notes, and PDFs, are unstructured data. Social media: Social media has a component of semi-structured data you can access through built-in analytics, but the content of each social media message is unstructured. Websites and markup language: The content on the web may be tagged, but code is not designed to capture the meaning or function of tagged elements in ways that support automated processing of the information contained on each page. XML provides an element of structure, however, these building blocks are filled with unstructured elements. Mobile and communications data: Your customer service and sales team are collating unstructured data in their phone calls and chat logs, including text messages, phone recordings, collaboration software, conferencing, and instant messaging. Survey responses: Every time you gather feedback from your customers, you’re collecting unstructured data. For example, surveys with text responses and open-ended comment fields are unstructured data. Scientific data: Field surveys, space exploration, seismic imagery, atmospheric data, topographic and weather data, and medical data. While these may have a base structure for collection, the data itself is often unstructured and requires thoughtful analysis. Machine and sensor data: billions of small files from IoT devices and business systems outputting information into log files are not consistent in a structured data manner. Semi-structured data This type of information has some organizing properties, making it easier to parse and analyze. Specifically, semi-structured data contains internal tags and markings that allow for grouping and hierarchies. Basically, semi-structured data is unstructured data with some metadata attached to it. The most typical example of semi-structured data is e-mail, because even though it cannot be stored in relational databases (columns and rows), it does have native metadata that allows for basic classification and keyword searches. Other typical examples are: the XML markup language, the versatile JSON data-interchange format, and databases of the NoSQL or non-relational variety. Even images could be considered semi-structured data: let’s say you take a picture of something with your phone. It automatically logs the time the picture was taken, the GPS data at the time of the capture and your device ID. If you’re using any kind of web service for storage, like iCloud, your account info becomes attached to the file. The actual content (the photo pixels) are not structured, but there are certain components that allow data to be grouped based on certain characteristics. 1.5 How is Big Data being used? Until recently, the only type of data that was considered was structured data – we were limited to spreadsheets or databases. Anything that couldn’t be easily organized into rows and columns was simply too difficult to work with and was simply ignored. However, advances in storage and analytics mean that we can now capture, store, and work with a wide range of data, both structured and unstructured. To make sense of all of this jumbled data, Big Data projects frequently employ cutting-edge analytics such as artificial intelligence and machine learning. By teaching computers to recognize what this data represents – for example, through image recognition or natural language processing – they can learn to spot patterns much faster and more reliably than humans. We will be focusing on Machine Learning tools in this course! Because of the ever-increasing stream of sensor data, photographs, text, voice, and video data, we can now use data in ways that were not even possible a few years ago. This is transforming the business world in almost every industry. Companies can now predict what specific segments of customers will want to buy and when they will want to buy it. Big Data is also assisting businesses in running their operations in a much more efficient manner. Even outside of business, Big Data projects are already influencing our world in a variety of ways, including, but not limited to: Improving healthcare. Data-driven medicine entails analyzing massive amounts of medical records and images for patterns that can aid in the early detection of disease and the development of new medicines. Predicting and responding to natural and man-made disasters. Sensor data can be analyzed to predict where earthquakes will strike next, and human behavior patterns provide clues that help organizations provide relief to survivors. Big Data technology is also used to track and protect the flow of refugees fleeing war zones around the world. Preventing crime. Police departments are increasingly implementing data-driven strategies based on their own intelligence and public data sets in order to deploy resources more efficiently and act as a deterrent where one is required. 1.6 Concerns Big Data provides us with previously unimaginable insights and opportunities, but it also raises concerns and questions that must be addressed. Privacy. Big Data can contain a great deal of information about our personal lives, much of which we have the right to keep private. We are increasingly being asked to strike a balance between the amount of personal information we share and the convenience that Big Data-powered apps and services provide. Data security. Even if we agree to give someone our data for a specific purpose, can we trust them to keep it safe? Data discrimination. Is it acceptable to discriminate against people based on data we have on their lives? Credit scoring is already used to determine who can borrow money, and insurance is heavily data-driven. "],["case-studies-on-big-data.-what-makes-big-data-valuable.html", "Chapter 2 Case studies on Big Data. What makes Big Data valuable? 2.1 ASSIGNMENT #1: PICK A CASE STUDY 2.2 An example: Netflix. 2.3 Other case studies.", " Chapter 2 Case studies on Big Data. What makes Big Data valuable? Broadly speaking, we can think of several main areas where Big Data is being used in practice: Customer targeting. Big data is used to better understand customers and their behaviors and preferences. Typical examples include Walmart (1, 2) and Target (1, 2), who could predict their customers’ pregnancies and offer tailored coupon books. Business process optimization. From supply chain to delivery route optimization, businesses are optimizing their logistics and stocks using geographic positioning and radio frequency identification (RFID) sensors to track goods or delivery vehicles and optimize routes by integrating live traffic data, etc (see for example). HR business processes are also being improved using big data analytics. Healthcare. Big data analytics computing power allows us to decode entire DNA strings in minutes, enabling us to find new cures and better understand and predict disease patterns. Consider what happens when all of the individual data collected by smart watches and wearable devices is applied to millions of people and their various diseases. Clinical trials in the future will not be limited by small sample sizes, but may include everyone. (read some examples) Sports performance. The majority of elite sports have now adopted big data analytics. We have the IBM SlamTracker tool for tennis tournaments; we use video analytics to track the performance of every player in a football or baseball game; and sensor technology in sports equipment such as basket balls or golf clubs allows us to get feedback on our game and how to improve it (via smart phones and cloud servers). Many elite sports teams track athletes even outside of the athletic environment, using smart technology to track nutrition and sleep, as well as social media conversations to monitor emotional well-being. Science and research. The CERN, the nuclear physics lab with the world’s largest and most powerful particle accelerator, has 65,000 processors to analyze the data center’s 30 petabytes of data. To analyze the data, however, it employs the computing power of thousands of computers distributed across 150 data centers worldwide (link). Such computing power has the potential to transform many other fields of science and research (link) However, Big Data computing power could be applied to any set of data, providing scientists with new sources. Researchers can now more easily access and analyze Census data and other government-collected data to create bigger and better pictures of our health and social sciences. Machine and device performance optimization. Big Data tools (specifically, advanced Neural Networks models) are used to operate Google’s self driving car (and so do other brands such as Toyota or Tesla). In Ireland, grocery chain Tesco has its warehouse employees wear armbands that track the goods they take from the shelves, distributes tasks, and even forecasts completion time for a job. Security and law enforcement. Law enforcement agencies all over the world already use Big Data to foil terrorist plots (e.g. the NSA), predict criminal activity or catch them. Credit card companies use Big Data to detect fraudulent transactions. City organization. The City of London uses Big Data to organize their public transport system; Los Angeles uses road sensors and traffic camaras to control traffic lights and manipulate the flow/congestion of traffic; Portugal has waste management sensors in some cities. Financial trading. Today, the majority of equity trading is done through data algorithms that use signals from social media networks and news websites to make buy and sell decisions in split seconds (link). 2.1 ASSIGNMENT #1: PICK A CASE STUDY Let’s dive deeper into a few Big Data case studies. I want you to each pick one Big Data case study and prepare a short presentation that you will deliver on January 17th on the following key points: Some background info on the case study. What problem is Big Data helping to solve? How are they actually using Big Data in practice? What were the results of implementing Big Data analysis in this case? What are the technical details? What are the challenges they had to overcome? Key takeaways and insights? 2.2 An example: Netflix. 2.2.1 Background The streaming service Netflix accounts for 35% of peak-time Internet traffic in the US, and the service now has 222 million subscribers in over 190 countries enjoying more than 6 billion hours of TV shows and movies per month. This amounts to almost 300GB of streaming per user per month. Data from these millions of subscribers is collected and monitored in an attempt to understand our viewing habits. 2.2.2 What problem is Big Data helping to solve? Netflix wants to predict what exactly subscribers will enjoy watching, and they’ve built their business model around this. 2.2.3 How is Netflix using Big Data in practice? A quick glance at their jobs posts page tells us that they take their Big Data very seriously. However, at their core, their business has consisted of them correctly predicting what users will enjoy watching. Big Data analytics is the fuel that fires the “recom- mendation engines” designed to serve this purpose. This started in 2006 (whilst they were still a DVD subscription business; streaming began a year later) with the launch of the $1 million Netflix prize to whomever could come up with the best algorithm to predict how their customers would rate a movie based on their previous ratings. The winning entry was announced in 2009 and key principles of it are still used. Note: you can check out the dataset used here at Kaggle and read more on the Netflix prize here. Analysts were initially constrained by a lack of information on their customers, with only four data points (customer ID, movie ID, rating, and date watched) available for analysis. As streaming became the primary method of delivery, many new data points about their customers became available (e.g. now they can observe when someone stops a movie or replays a section – or only watches two minute of it and never comes back). With this new information, Netflix was able to create models that predicted the perfect storm scenario of customers being consistently served with movies they would enjoy. After all, satisfied customers are far more likely to renew their subscriptions. Tagging is another important component of Netflix’s attempt to provide us with films that we will enjoy. The company pays people to watch movies and then tag them with movie elements. They will then recommend that you watch other productions that are tagged similarly to those you are watching. This is where the sometimes unusual “suggestions” come from. Netflix’s ultimate goal is to increase the number of hours customers spend using their service. Statistics aren’t really needed to tell you that viewers who don’t spend much time using the service are likely to feel they aren’t getting value for money from their subscriptions and may cancel them. To that end, the impact of various factors on “quality of experience” is closely monitored, and models are developed to investigate how this affects user behavior. Calculations about data placement can be made to ensure an optimal service to as many homes as possible by collecting end-user data on how the physical location of the content affects the viewer’s experience. 2.2.4 What were the results of implementing Big Data analysis? 2.2.5 What data was used? Data on what titles customers watch, what time of day movies are watched, time spent selecting movies, how often playback is stopped (either by the user or due to network limitations), and ratings given feed the recommendation algorithms and content decisions. Netflix collects data on buffering delays (rebuffer rate) and bitrate (which affects picture quality), as well as customer location, to analyze quality of experience. 2.2.6 What are the technical details? Although their vast library of movies and TV shows is hosted in the cloud on Amazon Web Services (AWS), it is also mirrored by ISPs and other hosts around the world. In addition to improving user experience by reducing lag when streaming content around the world, this lowers costs for ISPs by saving them the cost of downloading data from the Netflix server before passing it on to viewers at home. Their catalog was said to be three petabytes in size in 2013. This massive amount of data is explained by the need to store many of their titles in up to 120 different video formats due to the variety of devices that support Netflix playback. It now amounts to more than 17,000 titles worldwide. Their systems used Oracle databases at first, but they switched to NoSQL and Cassandra to allow for more complex, Big Data-driven analysis of unstructured data. Kurt Brown, who leads Netflix’s Data Platform team, spoke at the Strata + Hadoop World conference about how the company’s data platform is constantly evolving. Netflix’s data infrastructure incorporates Big Data technologies such as Hadoop, Hive, and Pig, as well as traditional business intelligence tools such as Teradata and MicroStrategy. It also includes Lipstick and Genie, two open-source applications and services developed by Netflix. And, like the rest of Netflix’s core infrastructure, it is hosted in the AWS cloud. In the future, Netflix will investigate Spark for streaming, machine learning, and analytic use cases, as well as continue to develop new components for their own open-source suite. 2.2.7 What are the challenges they had to overcome? All of this data had to be quantified in some way in order to be available for computer analysis and thus unlock its value. Netflix accomplished this by paying teams of thousands of viewers to sit through hours of content, meticulously tagging elements they discovered. Nowadays, they have incorporated AI into this process by developing routines that can take a Jpeg snapshot of the content and analyze what is happening on screen using sophisticated technologies such as facial recognition and color analysis. These snapshots can be taken at predetermined intervals or whenever a user performs a specific action, such as pausing or stopping playback. For example, if it knows a user has a habit of switching off after watching gory or sexual scenes, it can recommend more sedate alternatives the next time they sit down to watch something. 2.2.8 Key takeaways and insights? For networks, distributors, and producers, predicting what viewers will want to watch next is big business (all roles that Netflix now fill in the media industry). Netflix has taken the lead, but competitors such as HBO, Amazon, Apple, … have their own analytics, even though Netflix is still leader. As time passes, we can expect to see continued innovation in predictive content programming, driven by fierce competition. Netflix really begun to lay the groundwork for “personalized TV.” 2.3 Other case studies. Here are some ideas for you to present next week (or you can find your own!). Amazon. Uber. Google. Facebook. LinkedIn. Walmart. The CERN. Public London Transport System. Rolls-Royce. Shell. Royal Bank of Scotland. US Immigrations and Customs. Walt Disney Parks and Resorts. BBC. Sprint. Kaggle. "],["the-big-data-workflow.html", "Chapter 3 The Big Data Workflow 3.1 Acquiring data 3.2 Preparing data 3.3 Analyzing data 3.4 Communicating results 3.5 Turning insights into action 3.6 What next?", " Chapter 3 The Big Data Workflow To address the challenges of big data, innovative technologies are needed. Parallel, distributed computing paradigms, scalable machine learning algorithms, and real-time querying are key to analysis of big data. Distributed file systems, computing clusters, cloud computing, and data stores supporting data variety and agility are also necessary to provide the infrastructure for processing of big data. Workflows provide an intuitive, reusable, scalable and reproducible way to process big data to gain verifiable value from it in and enable application of same methods to different datasets. Now that we’ve defined what Big Data is and how businesses and institutions can strategize around big data to begin building a purpose, let’s return to using data science to extract value from big data around the purpose or questions they defined. We can define data science as a multi-disciplinary craft that combines People teaming up around an application-specific Purpose that can be achieved through a Process, Big Data computing Platforms, and Programmability. More specifically, when we think about the PROCESS, we can think about five different steps: These are five distinct activities that depend on each other. Let’s summarize each activity further before we go into the details of each. Acquire includes anything that makes us retrieve data including; finding, accessing, acquiring, and moving data. It includes identification of and authenticated access to all related data. And transportation of data from sources to distributed files systems. It includes way to subset and match the data to regions or times of interest. As we sometimes refer to it as geo-spacial query. The next activity is Prepare data. We divide the ‘’prepare data’’ activity into two steps based on the nature of the activity. Namely, explore data and pre-process data. The first step in data preparation involves literally looking at the data to understand its nature, what it means, its quality and format. It often takes a preliminary analysis of data, or at least a sample of the data, to understand it. Once we know more about the data through exploratory analysis, the next step is the pre-processing of data for analysis. Pre-processing includes cleaning data, sub-setting or filtering data and creating data which programs can read and understand, such as modeling raw data into a more defined data model, or packaging it using a specific data format. If there are multiple data sets involved, this step also includes merging of multiple data sources, or streams. The prepared data would then be passed onto the Analyze step, which involves selection of analytical techniques to use, building a model of the data, and analyzing results. This step can take a couple of iterations on its own or might require data scientists to go back to steps one and two to get more data or package data in a different way. Step four includes the evaluation of analytical results, presenting them in a visual way, creating Reports that include an assessment of results with respect to success criteria. Activities in this step can often be referred to with terms like interpret, summarize, visualize, or post process. The last step brings us back to the very first reason we do data science, the purpose (to obtain value out of data!). Reporting insights from analysis and determining actions from insights based on the purpose you initially defined is what we refer to as the Act step. Note that this is an iterative process and findings from one step may require the previous step to be repeated with new information. Also, it’s very typical of job postings to be specialized in a single one (or maybe two) of these steps – you don’t have to be an expert in all of them! Let’s dive into each one of them a little bit further. 3.1 Acquiring data Step one in the (Big) Data Science process is data acquisition. The first step in acquiring data is to determine what data is available. Leave no stone unturned when it comes to finding the right data sources. You want to identify the right data related to your problem and make use of all the data that is relevant to your problem for your analysis. Leaving out even a small amount of important data can lead to incorrect conclusions. Data comes from many places, local and remote, in many varieties, structured and unstructured. There are many techniques and technologies for accessing these different types of data. For example, there is a large amount of data in conventional relational databases, similar to the big data structure of organizations. The preferred tool for accessing data in databases is Structured Query Language or SQL, which is supported by all relational database management systems. In addition, most database systems come with a graphical application environment that allows you to query and explore the data sets in the database. Data can also exist in files such as text files and Excel spreadsheets. Scripting languages are generally used to obtain data from files. A scripting language is a high-level programming language that can be general purpose or specialized for specific functions. Common scripting languages with support for processing files are Java Script, Python, PHP, Perl, R and MATLAB, and many others. An increasingly popular way to obtain data is from web sites. Web pages are written using a set of standards approved by a worldwide web consortium (W3C). This includes a variety of formats and services. One common format is Extensible Markup Language, or XML, which uses markup symbols or tabs to describe the content of a web page. Many websites also host web services that produce program access to their data (e.g. REST). NoSQL storage systems are increasingly being used to manage a variety of data types in big data. These data warehouses are databases that do not represent data in a table format with columns and rows as with conventional relational databases. Examples of these data warehouses include Cassandra, MongoDB and HBASE (check out a comparison among the three here). NoSQL data stores provide APIs to allow users to access data. These APIs can be used directly or in an application that needs to access the data. Additionally, most NoSQL systems provide data access via a web service interface, such a REST. An example of data acquisition: the WIFIRE project The WIFIRE project acquires data from a variety of sources: They store sensor data from weather stations in a relational database, and use SQL to retrieve this data from the database. This allows them to create models to identify weather patterns associated with potentially dangerous wind conditions. To determine whether a particular weather station is currently experiencing potentially dangerous wind conditions (known as Santa Ana conditions), they access real time data using a web socket service. At the same time, Tweets are retrieved using hashtags related to any fire that is occurring in the region. The Tweet messages are retrieved using the Twitter REST service. The idea is to determine the sentiment of these tweets using Natural Language Processing to see if people are expressing fear, anger, etc. about the nearby fire. The combination of sensor data and tweet sentiments helps to give a sense of the urgency of the fire situation. 3.2 Preparing data This includes exploring and pre-processing data. Exploring data After gathering the data required for your application, you may be tempted to immediately create models to analyze the data right away. Avoid this temptation. The first step after receiving your data is to investigate it. Data exploration is one of two steps in the data preparation process. To gain a better understanding of the specific characteristics of your data, you should conduct preliminary research. This step will involve searching for correlations, general trends, and outliers. Without this step, you will be unable to effectively use the data. Correlation plots can be used to investigate the relationships between different variables in data. Plotting the general trends of the variables (e.g. time-trends) will show you whether there is a consistent direction in which the values of these variables are moving, such as sales prices increasing or decreasing over time. Plotting outliers will assist you in double-checking for measurement errors in the data. Outliers that are not errors may cause you to discover a rare event in some cases. Summary statistics also provide numerical values to describe the data. Mean, median, range, and standard deviation are some basic summary statistics you should compute for your data set. These metrics will give you an idea of the nature of your data, or they can tell you if there is an issue with your data. For example, if the data’s age range includes negative numbers or a number much greater than 100, there is something suspicious in the data that should be investigated. In this preliminary analysis step, visualization techniques also provide a quick, efficient, and generally very useful way to look at the data: A heat map can quickly show you where the hot spots are. Histograms depict the data distribution and can reveal unusual skewness or scatter. Boxplots are another type of plot used to depict data distribution. Line plots can show how data values change over time. Data spikes are also easy to spot. Scatter plots can demonstrate the relationship between two variables. … What you gain by exploring your data is a better understanding of the complexity of the data you have to work with. This, in turn, will guide the rest of your process. Pre-processing data The raw data you obtain directly from your sources is never in the format required for analysis. The data preprocessing step has two primary goals: The first step is to CLEAN the data in order to address data quality issues, and the second step is to TRANSFORM the raw data so that it can be analyzed. Cleaning the data Addressing quality issues in your data is a critical part of data preparation. Real-world data is disorganized. There are numerous examples of quality issues with real-world application data, such as: Inconsistent data, e.g. a customer with two addresses; Missing values, e.g. missing information on some of the key variables/demographics; Duplicate values, e.g. in customer records, a customer’s address are recorded in two different locations and the two recordings contradict each other…; Invalid data, such as a six-digit zip code (although this should be avoided with proper processes); Outliers, though we have to differentiate between a rare event (a true outlier, properly recorded) and simply a typo/misrecording. Depending on the project, we’ll probably have little control over how the data is collected because we get it downstream. Avoiding data quality issues as they arise is not always an option. So now that we have the data, we must address quality issues by detecting and correcting them. Here are some approaches to addressing these quality issues: We can get rid of data records that have missing values (though this might result in …?) Duplicate records can be combined. This will require a method for determining how to reconcile competing values. Depending on the project, it may make sense to keep the newer value (e.g. financial aid cases). In the case of invalid values, the best estimate for a reasonable value can be used. For example, if an employee’s age is missing, a reasonable value can be estimated based on the employee’s length of employment. Again, this is typically an arbitrary decision and any steps taken here must be properly documented and argued. Outliers can also be removed if they are irrelevant to the task. In general, to effectively address data quality issues, it is necessary to have knowledge of the application, such as how the data were collected, the user population, and the intended uses of the application. This domain knowledge is required to make informed decisions about how to deal with incomplete or incorrect data. Transforming the data The second stage of data preparation involves transforming the cleaned data into the format required for analysis. This step can take on several different names (Data manipulation, data pre-processing, data wrangling, and even data munging). In this part of the process, the data engineer takes on several different types of operations, such as: * Scaling, * Transformation, * Feature (variable) selection, * Dimensionality reduction, * Data manipulation. Scaling is the process of changing the range of values to be between a given range. For example, from zero to one. This is done to prevent certain characteristics with high values from dominating the results. For instance, in the analysis of data containing height and weight. The magnitude of the weight values is significantly greater than the magnitude of the height values. Scaling all values between zero and one equalizes the contributions of height and weight characteristics. This is typically done by standardization, i.e. calculating the z-score (subtracting the mean value and dividing it by its standard deviation). Several data transformations can be applied to reduce noise and variability. Aggregation is one such transformation. Aggregated data generally produces less variable data, which can aid in analysis. Daily sales figures, for example, can fluctuate dramatically. Adding values to weekly or monthly sales figures yields comparable results. Other filtering techniques can also be used to remove data variability. This, of course, comes at the expense of less detailed data. As a result, these factors must be considered for the specific application. Feature (variable) selection may include the removal of redundant or irrelevant entities, the combination of entities, and the creation of new entities. You may have discovered that two entities are related during the data exploration step. In that case, one of these features (variable) can be removed without affecting the analysis’s results. The purchase price of a product, for example, is likely to be correlated with the amount of sales tax paid. It will then be advantageous to eliminate the amount of sales tax. Eliminating redundant or irrelevant functions simplifies further analysis. In other cases, you may want to combine or create new features. It would make sense, for example, to include the applicant’s educational level as a feature in a loan approval request. There are also algorithms that can automatically determine the most important characteristics based on mathematical properties. When a data set has a large number of dimensions, dimensionality reduction is useful. It entails identifying a smaller subset of dimensions that capture the majority of the variation in the data. This reduces the data dimensions while removing irrelevant entities and simplifying the analysis. Principal Component Analysis (PCA), is a popular dimensional reduction technique. Raw data frequently needs to be manipulated in order for it to be in the proper format for analysis. For example, we may want to capture price changes for a specific market segment, such as real estate or health care, from samples that record daily changes in stock prices. This would necessitate determining which stocks fall into which market segment. Grouping them and possibly calculating the mean, range, and standard deviation for each group. Data preparation is a very important part of the data science process. In fact, this is where you will spend most of your time in any data science effort. It can be a tedious process, but it is a crucial step. Always remember, garbage in, garbage out. If you don’t spend the time and effort to create good data for analysis, you won’t get good results no matter how sophisticated the analysis technique you are using. 3.3 Analyzing data After you have thoroughly prepared your data, the next step is to analyze it. Data analysis entails creating a model from your data, which is referred to as input data. The analysis technique constructs a model from the input data. The output data is what your model generates (e.g. the prediction). There are various types of problems, and thus various types of analysis techniques. We’ll be focusing on the main ones, which are regularization, classification (tree-based methods), and unsupervised methods such as clustering. The most typical one is when your model must predict a numerical value instead of a category; this task is a regression problem. Predicting the price of a stock is an example of regression. The stock price is a numerical value, not something requiring a classification. Estimating the weekly sales of a new product or predicting a test score are two other examples of problems that can be solved with regression analysis. We will learn specific ML regression-techniques. On the other hand, the goal of classification is to predict the category of the input data. In this case, predicting the weather as sunny, rainy, windy, or cloudy is an example of this. Another example is determining whether a tumor is benign or malignant. Because there are only two categories, this classification is known as a binary classification. Another classic example is recognizing handwritten digits as belonging to one of ten categories ranging from zero to nine. Lastly, the goal of clustering is to organize similar items into groups. However, the main distinction with the classification problem is that we do not know beforehand what those groups are or what the ``true’’ classification is. As seen here, one example is segmenting a company’s customer base into distinct segments for more effective targeted marketing. Modeling begins with selecting, depending on the type of problem, one of the techniques listed. The model is then built using the data you’ve prepared. Lastly, you apply the model to new data samples to validate it. Cross-validation tells you how well the model performs on the data that was used to build it. We’ll delve deeper into this when starting the ML section of the course (specifically the ML framework). However, just as an overview, ot is common practice to divide the prepared data into a set of data for constructing the model (training) and a set of data for evaluating the model after it has been built (testing). You can also use new data prepared in the same manner as the data used to build the model. The model’s evaluation (validation) is dependent on the analysis techniques used. For example, for each sample (data point) in your input data for classification and regression, you will have the ``true’’ correct output. Therefore, the model can be validated by comparing the correct output to the output predicted by the model. On the other hand, the groups formed as a result of clustering should be examined to see if they make sense for your application. For example, do the customer segments accurately represent your customer base? Are they beneficial in your targeted marketing campaigns? You will be able to determine the next steps after you have evaluated (validated) your model to get a sense of its performance on your data. Some questions to consider include whether the analysis should be performed with more data in order to improve model performance. Would using different data types be beneficial? Is it difficult, for example, to distinguish customers from different regions in your clustering results? Is it possible to generate finer-grained customer segments by including zip code in your input data? Do the analysis results point to a more in-depth examination of some aspect of the problem? Predicting sunny weather, for example, yields excellent results, but rainy weather forecasting yields only average results. This means you should look more closely at your examples for rainy weather. Perhaps you simply require more rainy weather data points, or perhaps there are some anomalies in those data points Perhaps there is some missing data that must be included in order to fully capture rainy weather. The ideal situation would be for your model to perform exceptionally well in terms of the success criteria established when you defined the problem at the start of the project. In that case, you’re ready to move on to communicating and acting on the findings of your analysis. Data analysis entails selecting the best technique for your problem, developing the model using cross-validation techniques, and analyzing the results. 3.4 Communicating results The fourth step in our data science process is to report the findings of our research. This is a critical step in communicating your findings and making a case for subsequent actions. It can take on different forms depending on your audience and should not be taken lightly. So, where to begin? The first step is to review the results of your analysis and decide what to present or report as the greatest value. These are the questions you should ask yourself when deciding what to present. What exactly is the punchline? What are the primary outcomes? What added value do these results provide, and what contribution can the model make to the application? How do the results stack up against the success criteria established at the start of the project? The answers to these questions should be included in your report or presentation, so make them the main themes and collect facts to support them. Keep in mind that not all of your outcomes will be positive. Your analysis may yield results that contradict your hypothesis, or it may yield inconclusive or perplexing results. You must also show these results. Some of these findings may be perplexing to domain experts, and inconclusive findings may necessitate further investigation. Remember that the purpose of reporting your findings is to determine the next step. All findings must be presented in order for informed decisions to be made. Visualization is an important tool for communicating your findings. Scatter plots, line graphs, heat maps, and other graphs are effective ways to visually present your results. However, this time, instead of plotting the input data, you plot the output data using similar tools. You should also consider having backup tables with details of your analysis in case someone wants to dig deeper into the results. You want to report your findings by presenting your results and main added value using interactive visualization tools. 3.5 Turning insights into action Now that we’ve evaluated the results of your analysis and generated reports on their potential value, the next step is to decide what action or actions to take based on the insights gained. Remember why we started collecting and analyzing data in the first place: to find actionable information within all of these data sets in order to answer questions or improve business processes. Is there something in your process, for example, that you should change to eliminate bottlenecks? Is there any information that should be added to your application to improve its accuracy? Should you divide your population into more defined groups for better targeted marketing? This is the first step toward putting ideas into action. Now that you’ve decided what action to take, the next step is to figure out how to carry it out. What is required to incorporate this action into your process or application? What steps should be taken to automate it? Stakeholders must be identified and included in the change process. We must monitor and measure the impact of the action on the process or application, just as we would with any other process improvement change. An evaluation results from assessing the impact. The next steps will be determined by evaluating the outcomes of the implemented action. Is there anything else that needs to be done to get even better results? What information must be reviewed? Is there anything else that needs to be looked into? Let us not forget, for example, what big data enables us to do. Real-time actions based on high-speed data streaming. We must define which aspects of our business require real-time action in order to influence operations or customer interactions. Once we’ve defined these real-time actions, we must ensure that there are automated systems or processes in place to carry them out, as well as failover in the event of a problem. Big data and data science are only useful if the insights can be translated into actions that are carefully defined and evaluated. 3.6 What next? We’ll be focusing on different steps of the data science process. Remember, in the end, you’ll probably specialize in one or the other! "],["hadoop-and-spark.html", "Chapter 4 Hadoop and Spark 4.1 What is Hadoop? 4.2 What is Spark? 4.3 What are the key differences between Hadoop and Spark? 4.4 Advantages and disadvantages 4.5 Intro to spark with sparklyr in R", " Chapter 4 Hadoop and Spark Apache Spark and Apache Hadoop are both popular, open-source data science tools offered by the Apache Software Foundation. Apache Spark is designed as an interface for large-scale processing, while Apache Hadoop provides a broader software framework for the distributed storage and processing of big data. Both can be used either together or as standalone services. 4.1 What is Hadoop? Apache Hadoop is a set of open-source modules and utilities designed to simplify the process of storing, managing, and analyzing large amounts of data. It was created in 2006 by software engineers Doug Cutting and Mike Cafarella to process large amounts of data, initially using its namesake file system and MapReduce, a programming model and processing engine promoted by Google in a 2004 technical paper. Hadoop provides a way to efficiently break up large data processing problems across different computers, run computations locally and then combine the results. The distributed processing architecture of the framework makes it simple to create big data applications for clusters of hundreds or thousands of commodity servers, known as nodes. Hadoop is made up of three main components: HDFS (Hadoop Distributed File System): HDFS serves as Hadoop’s storage layer. On HDFS, data is always stored in the form of data-blocks, with the default size of each data-block being 128 MB, which is configurable. Hadoop employs a master-slave architecture based on the MapReduce algorithm. NameNode and DataNode in HDFS follow a similar pattern. MapReduce: MapReduce is a Hadoop processing layer. Map-Reduce is a programming model that has two phases: Map Phase and Reduce Phase. It is intended for parallel processing of data distributed across multiple machines (nodes). Its role was reduced by YARN in the Hadoop implementation of MapReduce in 2013. YARN (yet another Resources Negotiator): In Hadoop, YARN is the job scheduling and resource management layer. Data stored on HDFS is processed and run using data processing engines such as graph processing, interactive processing, batch processing, and so on. With the help of this YARN framework, the overall performance of Hadoop is improved. Hadoop features Features 1. Open Source. Hadoop is an open source software framework. This means that it is freely available and that we can modify its source code to meet our needs. 2. Distributed processing HDFS distributes data across the clusters, therefore data is processed in parallel by MapReduce on a cluster of nodes. 3. Fault Tolerance. Hadoop is a fault-tolerant system. By default, each block creates three replicas across the cluster, but this can be changed as needed. So, if one of the nodes fails, we can recover data from the other node. Failures of nodes or tasks are automatically recovered by the framework. 4. Reliability. It reliably stores data on the cluster despite machine failure. 5. High Availability. Despite hardware failure, data is highly available and accessible. When a machine or piece of hardware fails in Hadoop, we can still access data from another path. 6. Scalability. Hadoop is highly scalable because new hardware can be added to nodes. 7. Economical. Hadoop runs on a cluster of inexpensive commodity hardware. We don’t need any specialized equipment for it. 8. Simple to use. There is no need for the client to deal with distributed computing because the framework handles everything. 4.2 What is Spark? Apache Spark is a free and open-source data processing engine designed for large-scale data analysis. Apache Spark, a powerful unified analytics engine, is frequently used by data scientists to support ML algorithms and complex data analytics. Apache Spark can be used as a standalone application or as a software package on top of Apache Hadoop. Spark was created by Matei Zaharia in 2009 while he was a graduate student at the University of California, Berkeley. His main contribution to the technology was to improve how data is organized in order to more efficiently scale in-memory processing across distributed cluster nodes. Spark, like Hadoop, can process massive amounts of data by distributing workloads across multiple nodes, but it typically does so much faster. This allows it to handle use cases that Hadoop cannot handle with MapReduce, transforming Spark into a more general-purpose processing engine. Spark features Open source. Fault Tolerance. Apache Spark is built to deal with worker node failures. It achieves fault tolerance by utilizing DAG and RDD (Resilient Distributed Datasets). A DAG stores the history of all the transformations and actions required to complete a task. In the event of a worker node failure, rerunning the steps from the existing DAG yields the same results. Dynamic nature. It has over 80 high-level operators that make it simple to build parallel apps. Lazy evaluation. Spark does not evaluate transformations immediately because they are lazy. All transformations are evaluated slowly. The transformations are added to the DAG, and the final computation or results are only available when actions are executed. This makes it extremely fast and lightweight. Speed. Spark allows Hadoop applications to run up to 100x faster in memory and up to 10x faster on disk. Spark accomplishes this by reducing the number of disk read/write operations for intermediate results. It only stores data in memory and performs disk operations when necessary. Spark accomplishes this through the use of a DAG, a query optimizer, and a highly optimized physical execution engine. Reusability. Spark code can be used for batch processing, joining streaming data with historical data, and running ad-hoc queries on the state of a streaming stream. Advanced analytics. Spark has quickly become the de facto standard for big data processing and data science across multiple industries. Spark includes machine learning and graph processing libraries, which are used by businesses across industries to solve complex problems. In Memory Computing. Unlike Hadoop MapReduce, Spark can process tasks in memory and does not require intermediate results to be written back to disk. This feature significantly accelerates Spark processing. In addition, Spark is capable of caching intermediate results so that they can be reused in the next iteration. This gives Spark an additional performance boost for iterative and repetitive processes where results from one step can be used later or there is a shared dataset that can be used across multiple tasks. Supporting Multiple languages: Spark includes multi-language support. The majority of the APIs are available in Java, Scala, Python, and R. There are also advanced data analytics features available with the R language. Spark also includes SparkSQL, which is a SQL-like feature. SQL developers find it very easy to use, and the learning curve is greatly reduced. Integrated with HADOOP. Apache Spark integrates extremely well with the Hadoop file system HDFS. It supports a variety of file formats, including parquet, json, csv, ORC, and Avro. Hadoop can be easily integrated with Spark as an input or destination data source. Cost-effective. Because Apache Spark is open source software, there is no licensing fee associated with it. Users only need to be concerned about the hardware cost. Additionally, Apache Spark reduces other costs because it includes built-in support for stream processing, machine learning, and graph processing. Spark does not have any vendor lock-in, making it very easy for organizations to pick and choose Spark features based on their use case. 4.3 What are the key differences between Hadoop and Spark? Apache Hadoop and Apache Spark, as well as other data science tools, may be used pretty much interchangeably. The use of MapReduce by Hadoop is the most notable difference between the two frameworks. In the early versions of Hadoop, HDFS was linked to it, whereas Spark was created specifically to replace MapReduce. Even though Hadoop no longer relies solely on MapReduce for data processing, there is still a strong link between the two. When it comes to keeping costs down for large processing jobs that can tolerate some delays, MapReduce in Hadoop has advantages. Because it is designed to process data mostly in memory, Spark has a clear advantage over MapReduce in delivering timely analytics insights. Hadoop was the first to arrive, and it changed the way people thought about scaling data workloads. It enabled organizations to implement big data environments with large volumes and diverse types of data, particularly for aggregating and storing data sets to support analytics applications. As a result, it’s frequently used as a platform for data lakes, which commonly store both raw data and prepared data sets for analytics. While Hadoop can now be used for more than just batch processing, it is best suited for historical data analysis. Spark was built from the ground up to handle high-throughput data processing tasks. As a result, it is suitable for a variety of applications. Spark is used in online applications, interactive data analysis, extract, transform, and load (ETL), and other batch processes. It can be used to analyze data on its own or as part of a data processing pipeline. Spark can also be used on top of a Hadoop cluster as a staging tier for ETL and exploratory data analysis. This highlights another significant distinction between the two frameworks: Because Spark lacks a built-in file system like HDFS, it must be used in conjunction with Hadoop or other platforms for long-term data storage and management. 4.4 Advantages and disadvantages Choose Spark if you are a data scientist who primarily works with machine learning algorithms and large-scale data processing. Spark: * Runs as a stand-alone utility independent of Apache Hadoop. * Distributed task dispatching, I/O functions, and scheduling are all available. * Multiple languages are supported, including Java, Python, and Scala. * Provides implicit data parallelism as well as fault tolerance. Choose Hadoop if you are a data scientist who needs a wide range of data science utilities for big data storage and processing. Hadoop: * Provides a comprehensive framework for big data storage and processing. * Provides a huge selection of packages, including Spark itself. * Is based on a distributed, scalable, and portable file system. * Utilizes additional data warehousing, machine learning, and parallel processing applications. 4.5 Intro to spark with sparklyr in R You’ll now start practicing with the sparklyr package in R through DataCamp: Introduction to spark with sparklyr in R. "],["machine-learning-for-social-scientists.-intro-to-the-ml-framework..html", "Chapter 5 Machine Learning for Social Scientists. Intro to the ML Framework. 5.1 Prediction vs inference 5.2 Split your data into training/testing 5.3 Cross-validation 5.4 Bias-variance tradeoff 5.5 An example", " Chapter 5 Machine Learning for Social Scientists. Intro to the ML Framework. 5.1 Prediction vs inference 5.2 Split your data into training/testing 5.3 Cross-validation 5.4 Bias-variance tradeoff 5.5 An example "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
